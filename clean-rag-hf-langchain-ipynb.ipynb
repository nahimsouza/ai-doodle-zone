{"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/huggingface/cookbook/blob/main/notebooks/en/advanced_rag.ipynb","timestamp":1719000657370}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RAG with Hugging Face + LangChain\n","metadata":{"id":"hUCaGdAj9-9F"}},{"cell_type":"markdown","source":"## Install libraries","metadata":{"id":"cL-MKHnC3McU"}},{"cell_type":"code","source":"!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille","metadata":{"id":"NSX0p0rV9-9I","execution":{"iopub.status.busy":"2024-06-25T18:17:35.788456Z","iopub.execute_input":"2024-06-25T18:17:35.788893Z","iopub.status.idle":"2024-06-25T18:17:53.266521Z","shell.execute_reply.started":"2024-06-25T18:17:35.788862Z","shell.execute_reply":"2024-06-25T18:17:53.264760Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Define parameters","metadata":{"id":"Y2Q8cVQH8zSc"}},{"cell_type":"code","source":"# Dataset used to provide context for the answers\n# From Hugging Face: https://huggingface.co/datasets/m-ric/huggingface_doc\nDATASET_NAME = \"m-ric/huggingface_doc\"\n\n# Model that will embed the documents into the Vector DB\n# From Hugging Face: https://huggingface.co/thenlper/gte-small\nEMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n\n# LLM that will read the prompt with the context and provide an output\n# From Hugging Face: https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\nREADER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n\n# Model for RAGatoulie library - https://github.com/bclavie/RAGatouille\n# From Hugging Face: https://huggingface.co/colbert-ir/colbertv2.0\nRERANKER_MODEL_NAME = \"colbert-ir/colbertv2.0\"","metadata":{"id":"eisIN_Dt86eM","execution":{"iopub.status.busy":"2024-06-25T18:17:53.269746Z","iopub.execute_input":"2024-06-25T18:17:53.270236Z","iopub.status.idle":"2024-06-25T18:17:53.277246Z","shell.execute_reply.started":"2024-06-25T18:17:53.270190Z","shell.execute_reply":"2024-06-25T18:17:53.276104Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Prepare functions","metadata":{"id":"-uS6Mv8O9-9L"}},{"cell_type":"code","source":"from typing import List, Optional, Tuple\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom transformers import AutoTokenizer\n\ndef split_documents(\n    chunk_size: int,\n    knowledge_base: List[LangchainDocument],\n    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n) -> List[LangchainDocument]:\n    \"\"\"\n    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n    \"\"\"\n\n    # We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n    # This list is taken from LangChain's MarkdownTextSplitter class\n    MARKDOWN_SEPARATORS = [\n        \"\\n#{1,6} \",\n        \"```\\n\",\n        \"\\n\\\\*\\\\*\\\\*+\\n\",\n        \"\\n---+\\n\",\n        \"\\n___+\\n\",\n        \"\\n\\n\",\n        \"\\n\",\n        \" \",\n        \"\",\n    ]\n\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=MARKDOWN_SEPARATORS,\n    )\n\n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed += text_splitter.split_documents([doc])\n\n    # Remove duplicates\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n\n    return docs_processed_unique\n","metadata":{"id":"9hvIL2jO9-9M","execution":{"iopub.status.busy":"2024-06-25T18:19:32.593091Z","iopub.execute_input":"2024-06-25T18:19:32.593500Z","iopub.status.idle":"2024-06-25T18:19:32.606729Z","shell.execute_reply.started":"2024-06-25T18:19:32.593469Z","shell.execute_reply":"2024-06-25T18:19:32.605724Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from typing import List, Optional, Tuple\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.vectorstores import FAISS\nfrom ragatouille import RAGPretrainedModel\nfrom transformers import Pipeline\n\ndef answer_with_rag(\n    question: str,\n    llm: Pipeline,\n    knowledge_index: FAISS,\n    prompt_template: str,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 5,\n) -> Tuple[str, List[LangchainDocument]]:\n\n    # Gather documents with retriever\n    print(\"=> Retrieving documents...\")\n    relevant_docs = knowledge_index.similarity_search(\n        query=question, k=num_retrieved_docs\n    )\n    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n\n    # Optionally rerank results\n    if reranker:\n        print(\"=> Reranking documents...\")\n        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n\n    relevant_docs = relevant_docs[:num_docs_final]\n\n    # Build the final prompt\n    context = \"\\nExtracted documents:\\n\"\n    context += \"\".join(\n        [f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)]\n    )\n\n    final_prompt = prompt_template.format(question=question, context=context)\n\n    # Redact an answer\n    print(\"=> Generating answer...\")\n    answer = llm(final_prompt)[0][\"generated_text\"]\n\n    return answer, relevant_docs","metadata":{"id":"n11zYRfn9-9O","execution":{"iopub.status.busy":"2024-06-25T18:19:36.172550Z","iopub.execute_input":"2024-06-25T18:19:36.173262Z","iopub.status.idle":"2024-06-25T18:19:58.093170Z","shell.execute_reply.started":"2024-06-25T18:19:36.173227Z","shell.execute_reply":"2024-06-25T18:19:58.092093Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2024-06-25 18:19:47.348181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-25 18:19:47.348314: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-25 18:19:47.452841: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Load the knowledge base\n\n","metadata":{"id":"Kr6rN10U9-9J"}},{"cell_type":"code","source":"import datasets\nfrom tqdm.notebook import tqdm\nfrom langchain.docstore.document import Document as LangchainDocument\n\nds = datasets.load_dataset(DATASET_NAME, split=\"train\")\n\nRAW_KNOWLEDGE_BASE = [\n    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n    for doc in tqdm(ds)\n]","metadata":{"id":"qZLVIEVW9-9J","execution":{"iopub.status.busy":"2024-06-25T18:20:23.497687Z","iopub.execute_input":"2024-06-25T18:20:23.498852Z","iopub.status.idle":"2024-06-25T18:20:24.450227Z","shell.execute_reply.started":"2024-06-25T18:20:23.498814Z","shell.execute_reply":"2024-06-25T18:20:24.448470Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2647 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edeb1c868e8049208b772f37a38bc841"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Build the vector database\n\nðŸš¨ðŸ‘‡ The cell below takes a few minutes to run on A10G!","metadata":{"id":"J1ho-UKM9-9M"}},{"cell_type":"code","source":"from langchain.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\n# Split the documents\ndocs_processed = split_documents(\n    512,  # We choose a chunk size adapted to our model\n    RAW_KNOWLEDGE_BASE,\n    tokenizer_name=EMBEDDING_MODEL_NAME,\n)\n\n# Prepare embedding model\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)\n\n# Create an embedding of each doc chunk and store the embedding in the vector database\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)","metadata":{"id":"dalledM99-9M","execution":{"iopub.status.busy":"2024-06-25T18:20:30.717326Z","iopub.execute_input":"2024-06-25T18:20:30.717724Z","iopub.status.idle":"2024-06-25T18:23:56.289060Z","shell.execute_reply.started":"2024-06-25T18:20:30.717676Z","shell.execute_reply":"2024-06-25T18:23:56.287987Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715cd33328a94b5382b9f1289aa5171a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fcda785bf3e42c5aa0c1f2fcdb5a699"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015bece1a2e2492b9ef9bd946f12789d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23a152489f1b4cadbee88a52226272a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"466cc231c5bb4b3d9b9aedc681cdbbc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1d007e62d754a26937f5ec000dc143d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0623eb0de1fe410a97f0dd8ece845dba"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07311a08029456c85facf462f4e48ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c882bbb6e0f04aadabc3ec0ea7f07f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8ee5b569d34331889cbc1a8ddd4a49"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare Reader model\n","metadata":{"id":"0xiXcG269-9N"}},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n\n# Load the quantized version of the model to make inferences faster\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    READER_MODEL_NAME, quantization_config=bnb_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n\nREADER_LLM = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    do_sample=True,\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=500,\n)\n\n# Call LLM pipeline\n# READER_LLM(\"What is 4+4? Answer:\")","metadata":{"id":"QX_ORK4l9-9N","execution":{"iopub.status.busy":"2024-06-25T18:24:29.675397Z","iopub.execute_input":"2024-06-25T18:24:29.675825Z","iopub.status.idle":"2024-06-25T18:26:41.999095Z","shell.execute_reply.started":"2024-06-25T18:24:29.675792Z","shell.execute_reply":"2024-06-25T18:26:41.998130Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96e34222669b40cba193dc89375c2665"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72acb33d9196425983d8ab200b7f6092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62cc4d86af504b11a03bddb6ef244e32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b655dc065ff4fd49fa4532a290f8b43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"635d49b6372f408d867caafc30a1ba6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab5557b7900411387718ca56a19b557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9671b0ba8c24249b74e586385ed3d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ae3eb82bc0478bbad3f3d8297ae344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093354d963ee4587882e7fdf372d9401"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5408ab9d81483685776751b353df10"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6edf2b30944242b8a429c559f7f209cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"535ff1c421b24cdb9746718c3344f60f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb82793d3c7143fbae4fdd7140225bed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66437abf4fdf49099768877599216fb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d22a98d3e2a04ba3b1a1ef42b5d34cdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b27c5e579c464e6da43a25d3b3fcd400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4224f70b40c4fe1bb27aa8008721f92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"937d4be4650442e5b4b0f34cce885789"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Prepare the prompt\n","metadata":{"id":"RlfHavRT9-9O"}},{"cell_type":"code","source":"prompt_in_chat_format = [\n    {\n        \"role\": \"system\",\n        \"content\": \"\"\"Using the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.\"\"\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"\"\"Context:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}\"\"\",\n    },\n]\n\nRAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n)\n\nprint(RAG_PROMPT_TEMPLATE)","metadata":{"id":"Abn4gw5A9-9O","execution":{"iopub.status.busy":"2024-06-25T18:28:44.337503Z","iopub.execute_input":"2024-06-25T18:28:44.337942Z","iopub.status.idle":"2024-06-25T18:28:44.408884Z","shell.execute_reply.started":"2024-06-25T18:28:44.337909Z","shell.execute_reply":"2024-06-25T18:28:44.407719Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<|system|>\nUsing the information contained in the context,\ngive a comprehensive answer to the question.\nRespond only to the question asked, response should be concise and relevant to the question.\nProvide the number of the source document when relevant.\nIf the answer cannot be deduced from the context, do not give an answer.</s>\n<|user|>\nContext:\n{context}\n---\nNow here is the question you need to answer.\n\nQuestion: {question}</s>\n<|assistant|>\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Call the RAG model","metadata":{"id":"9nA4nwRQ9-9P"}},{"cell_type":"code","source":"from ragatouille import RAGPretrainedModel\n\nRERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL_NAME)\n\nquestion = \"how to create a pipeline object?\"\n\nanswer, relevant_docs = answer_with_rag(\n    question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, RAG_PROMPT_TEMPLATE, reranker=RERANKER\n)","metadata":{"id":"7ZTC1FtX9-9P","execution":{"iopub.status.busy":"2024-06-25T18:28:56.738615Z","iopub.execute_input":"2024-06-25T18:28:56.740342Z","iopub.status.idle":"2024-06-25T18:29:58.132034Z","shell.execute_reply.started":"2024-06-25T18:28:56.740303Z","shell.execute_reply":"2024-06-25T18:29:58.130854Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff093bd49dc4d81b80dd9d3b1c29520"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1bad038f63418d8d77810df8e29335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a331876ffee4230908ea5b78565ec33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c209fa37e7de47daa8b038d8241b117f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0185f757d0094896ab11bf144ebe7c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0595a66af0864d45800eb8edfd2e27fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06899d6afb384cb09e09d62fcad37b54"}},"metadata":{}},{"name":"stdout","text":"=> Retrieving documents...\n=> Reranking documents...\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"==================================Answer==================================\")\nprint(f\"{answer}\")\nprint(\"==================================Source docs==================================\")\nfor i, doc in enumerate(relevant_docs):\n    print(f\"Document {i}------------------------------------------------------------\")\n    print(doc)","metadata":{"id":"SwW0oqhZ9-9P","execution":{"iopub.status.busy":"2024-06-25T18:30:40.783528Z","iopub.execute_input":"2024-06-25T18:30:40.783936Z","iopub.status.idle":"2024-06-25T18:30:40.791077Z","shell.execute_reply.started":"2024-06-25T18:30:40.783906Z","shell.execute_reply":"2024-06-25T18:30:40.789965Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"==================================Answer==================================\nTo create a pipeline object, follow these steps:\n\n1. Import the necessary module, `pipeline`, from the Transformers library:\n\n   ```python\n   from transformers import pipeline\n   ```\n\n2. Specify the task you want to perform with the pipeline. Here are some examples:\n\n   - Object detection:\n\n     ```python\n     >>> object_detector = pipeline('object-detection')\n     ```\n\n   - Sentiment analysis:\n\n     ```python\n     >>> classifier = pipeline(\"sentiment-analysis\")\n     ```\n\n   - Text-to-image generation:\n\n     ```python\n     >>> image = pipeline(\n    ...     \"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n    ... ).images[0]\n     ```\n\n3. You can also load pre-trained models using the `from_pretrained()` function, as shown below:\n\n   ```python\n   from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n   import torch\n\n   pipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n       \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n   )\n\n   pipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\n   ```\n\nIn summary, to create a pipeline object, import the `pipeline` function from the Transformers library, specify the task you want to perform, and optionally load pre-trained models using the `from_pretrained()` function.\n==================================Source docs==================================\nDocument 0------------------------------------------------------------\n# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object-detection')\n>>> object_detector(image)\n[{'score': 0.9982201457023621,\n  'label': 'remote',\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n {'score': 0.9960021376609802,\n  'label': 'remote',\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n {'score': 0.9954745173454285,\n  'label': 'couch',\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n {'score': 0.9988006353378296,\n  'label': 'cat',\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n {'score': 0.9986783862113953,\n  'label': 'cat',\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\nDocument 1------------------------------------------------------------\n# Allocate a pipeline for object detection\n>>> object_detector = pipeline('object_detection')\n>>> object_detector(image)\n[{'score': 0.9982201457023621,\n  'label': 'remote',\n  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n {'score': 0.9960021376609802,\n  'label': 'remote',\n  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n {'score': 0.9954745173454285,\n  'label': 'couch',\n  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n {'score': 0.9988006353378296,\n  'label': 'cat',\n  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n {'score': 0.9986783862113953,\n  'label': 'cat',\n  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\nDocument 2------------------------------------------------------------\nStart by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n\n```py\n>>> from transformers import pipeline\n\n>>> classifier = pipeline(\"sentiment-analysis\")\nDocument 3------------------------------------------------------------\n```\n\n2. Pass a prompt to the pipeline to generate an image:\n\n```py\nimage = pipeline(\n\t\"stained glass of darth vader, backlight, centered composition, masterpiece, photorealistic, 8k\"\n).images[0]\nimage\nDocument 4------------------------------------------------------------\n```\n\n## Use multiple pipelines\n\nFor some workflows or if you're loading many pipelines, it is more memory-efficient to reuse the same components from a checkpoint instead of reloading them which would unnecessarily consume additional memory. For example, if you're using a checkpoint for text-to-image and you want to use it again for image-to-image, use the [`~AutoPipelineForImage2Image.from_pipe`] method. This method creates a new pipeline from the components of a previously loaded pipeline at no additional memory cost.\n\nThe [`~AutoPipelineForImage2Image.from_pipe`] method detects the original pipeline class and maps it to the new pipeline class corresponding to the task you want to do. For example, if you load a `\"stable-diffusion\"` class pipeline for text-to-image:\n\n```py\nfrom diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\nimport torch\n\npipeline_text2img = AutoPipelineForText2Image.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, use_safetensors=True\n)\nprint(type(pipeline_text2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'>\"\n```\n\nThen [`~AutoPipelineForImage2Image.from_pipe`] maps the original `\"stable-diffusion\"` pipeline class to [`StableDiffusionImg2ImgPipeline`]:\n\n```py\npipeline_img2img = AutoPipelineForImage2Image.from_pipe(pipeline_text2img)\nprint(type(pipeline_img2img))\n\"<class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.StableDiffusionImg2ImgPipeline'>\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n\nðŸ’¡ __Many options could be considered here to further improve the results:__\n- Compress the retrieved context to keep only the most relevant parts to answer the query.\n- Extend the RAG system to make it more user-friendly:\n    - cite source\n    - make conversational","metadata":{"id":"w6iNo7lY9-9S"}}]}